name: Prompt Evaluation

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

jobs:
  evaluate:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
    
    - name: Install promptfoo
      run: npm install -g promptfoo
    
    - name: Run promptfoo evaluation
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        # Find the latest test cases file
        TEST_FILE=$(ls -t .specalign/test_cases/*.yaml 2>/dev/null | head -1)
        if [ -z "$TEST_FILE" ]; then
          echo "No test cases found. Please run 'specalign generate' first."
          exit 1
        fi
        echo "Running evaluation on: $TEST_FILE"
        # Create output directory
        mkdir -p promptfoo-output
        # Run evaluation - promptfoo saves to .promptfoo/ by default
        promptfoo eval -c "$TEST_FILE"
        # Copy results to our output directory for artifacts
        if [ -d .promptfoo ]; then
          cp -r .promptfoo/* promptfoo-output/ 2>/dev/null || true
        fi
        # Also generate JSON output for easier parsing
        promptfoo eval -c "$TEST_FILE" --output json > promptfoo-output/results.json 2>&1 || true
    
    - name: Upload results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: promptfoo-results
        path: |
          promptfoo-output/
          .promptfoo/
        retention-days: 30
    
    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          // Try to read summary from .promptfoo directory
          const resultsPath = '.promptfoo/results.json';
          const summaryPath = 'promptfoo-output/results.json';
          
          let summary = null;
          if (fs.existsSync(resultsPath)) {
            try {
              summary = JSON.parse(fs.readFileSync(resultsPath, 'utf8'));
            } catch (e) {
              console.log('Could not parse results.json');
            }
          } else if (fs.existsSync(summaryPath)) {
            try {
              summary = JSON.parse(fs.readFileSync(summaryPath, 'utf8'));
            } catch (e) {
              console.log('Could not parse promptfoo-output/results.json');
            }
          }
          
          if (summary && summary.results) {
            const total = summary.results.length;
            const pass = summary.results.filter(r => r.success).length;
            const fail = total - pass;
            const passRate = total > 0 ? ((pass / total) * 100).toFixed(1) : '0';
            
            const body = `## Prompt Evaluation Results\n\n` +
              `- **Total Tests**: ${total}\n` +
              `- **Passed**: ${pass}\n` +
              `- **Failed**: ${fail}\n` +
              `- **Pass Rate**: ${passRate}%\n\n` +
              `[View full results](${process.env.GITHUB_SERVER_URL}/${process.env.GITHUB_REPOSITORY}/actions/runs/${process.env.GITHUB_RUN_ID})`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });
          } else {
            console.log('No summary data found to comment');
          }
